---
layout: distill
title: LSTM Word Prediction from Scratch
date: 2023-11-01
description: This is a documentation for our work on constructing an LSTM net-work that uses only Numpy for word prediction. In our work, we use previous two words to predict a next word. The formulas and structure of our program are described in detail in this documentation.
tags: NN LSTM
categories: Scratch
authors:
  - name: Phuoc Bui
    url: "https://phuoc-bui.github.io"
    affiliations:
      name: Chidoanh
  - name: Thuong Dang
    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: IAS, Princeton
toc:
  - name: LSTM architecture
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Forward method
  - name: Backward method
  - name: Experiment
related_posts: false
giscus_comments: true
featured: false
bibliography: 2018-12-22-distill.bib
---
<d-cite key="gregor2015draw"></d-cite>
## LSTM architecture

In short, LSTM is a variant of RNN, and it has _memory cells_ and it _learns to forget_. That means, at each step, we will forget unimportant information and memorize important information. Let us look at the architecture of LSTM.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/lstm/LSTM.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    [1] LSTM cell internal structure.
</div>

The picture is taken from [1]. For each block of an LSTM network, there will be four gates: a forget gate, an input gate, an output gate, and an intermediate gate, and their values are denoted $f_t, i_t, o_t, \tilde{c_t}$, respectively. We also denote $c_t$ the internal memory state, and $h_t$ the hidden state as in the architecture of RNN. Let us take a explain a bit further on this gated architecture. For the inputs, we denote

$h_{t-1}$: the previous hidden state.

$s_t = W_{\_} x_t + b_{\_}$: where $W_{\_}$ and $b_{\_}$ denote the weight and bias for gate $\_$.

$c_{t-1}$: the previous memory cell.

For gates, we have

__Forget gate.__ As we can see, the output of the forget gate $f_t$ has inputs $h_{t-1}$ and $s_t$, where $s_t = W_f x_t + b_f$. Those are weight bias for the forget gate. And 

$$f_t = \sigma(U_f h_{t-1} + W_f x_t + b_f),$$
where $\sigma$ is the sigmoid function, $U_f$ is the weight for the forget gate of $h_{t-1}$.

__Input gate.__ The output of the input gate, denoted $i_t$ has inputs $h_{t-1}$ and $s_t$, where $s_t = W_i x_t + b_i$. Those are weight and bias for the input gate. And

$$i_t = \sigma(U_i h_{t-1} + W_i x_t + b_i),$$
where $U_i$ is the weight of $h_{t-1}$ for the input gate.

__Output gate.__ Similarly, we have
$$o_t = \sigma(U_i h_{t-1} + W_o x_t + b_o)$$

__Intermediate gate.__ We have
$$\tilde{c_t} = \tanh(U_c h_{t-1} + W_c x_t + b_c)$$
Through gates, we can define our memory cell
$$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t},$$

where $\odot$ denotes the Hadamard product, i.e. it is element-wise multiplication of matrices. Let us explain a bit about this. Our $f_t$ is defined as a sigmoid function of previous state and current input, and its range is between $0$ and $1$. With Hadamard product, if a value $f_{ij}$ of $f_t$ is very close to $0$, then $$(c_{t-1})_{ij}f_{ij}$$ is also very close to zero, and it means, the value of this position is not very important we can forget it. On the other hand, the product $i_t \odot \tilde{c_t}$ decides which information will be updated. Why? Because if a value $i_{jk}$ of $i_t$ is very close to $1$, then $$(i_t \odot \tilde {c_t})_{jk}$$ is very close $(\tilde{c_t})_{jk}$. In some variants of LSTM, $i_t$ is replaced by $(1 - f_t)$. In short, $c_t$ helps us filter information: it determines which information we should forget or remember.

Now, we can update our current hidden state
$$h_t = \tanh(c_t) \odot o_t$$

And again, we see a filter $o_t$ here: it decides which information to pass through. To produce an output, we can again use softmax

$$\widehat{y} = \operatorname{softmax}(W_y h_n + b_y),$$
where $h_n$ is the last hidden state.

## Forward method

In the forward method, we initialized 
    - random matrices $U_f, W_f, U_i, W_i, U_c, W_c, U_o, W_o$ and
    - random vectors $b_f, b_i, b_c, b_o$
for the four gates. Moreover, we will also initialize randomly the weight $W_y$ and the bias $b_y$ for the output. Their sizes depend on our input size, output size and a chosen hidden size. In our case, we set input size and output size to be the size of our dictionary, and hidden size 64.

Note that, because of the LSTM architecture, we will need to add the zero vector $h_0$ and $c_0$ for the initialized hidden state and memory cell. Because we will start from time $t=1$, and the length of hidden states and gates are the same, it is easier if we also set those vectors $f_0, i_0, \tilde{c_0}, o_0$ to be zero (we never use them though). Now, we can easily code the forward method based on the formulas above. Here are the codes

Our dataset is very simple because of limited resources. It consists of 40 random sentences generated by chatGPT. Our task is to predict a word based on two previous words. 

First, we created a dictionary consisting of all words in our dataset. With each sentence, we created pairs of inputs and outputs, where the inputs are two consecutive words, and outputs are the third consecutive words. For example, if we have a sentence "I am so happy", then there will be two pairs $X_0 = \text{"i am"}, y_0 = \text{"so"}$, and $X_1 = \text{"am so"}, y_1 = \text{"happy"}$. 

We then transformed pairs into one-hot-encoded vectors, whose lengths are equal to the size of our dictionary. Of course there are other methods for word embeddings, such as word2vec or GloVe but it is not our main purposes. That's why we keep it simple. 

Here are the results of our model

As we can see, the results is quite low because of the following reasons:
- The dataset consists of random sentences and does not have a specific context.
- The size of the dataset is very small.
- The word prediction task is hard, especially with two or more words.
- The program is for educational purpose.

